import pandas as pd
import requests
import time
import warnings
import os
from io import StringIO
from bs4 import BeautifulSoup
from akshare.utils.cons import headers

warnings.filterwarnings("ignore")

def sw_index_third_info() -> pd.DataFrame:
    """è·å–æ‰€æœ‰ç”³ä¸‡ä¸‰çº§è¡Œä¸šä»£ç ï¼ˆç”¨äºéå†æŠ“å–å…¨Aè‚¡ï¼‰"""
    url = "https://legulegu.com/stockdata/sw-industry-overview"
    r = requests.get(url, headers=headers)
    soup = BeautifulSoup(r.text, features="lxml")
    code_raw = soup.find(name="div", attrs={"id": "level3Items"}).find_all(
        name="div", attrs={"class": "lg-industries-item-chinese-title"}
    )
    name_raw = soup.find(name="div", attrs={"id": "level3Items"}).find_all(
        name="div", attrs={"class": "lg-industries-item-number"}
    )
    value_raw = soup.find(name="div", attrs={"id": "level3Items"}).find_all(
        name="div", attrs={"class": "lg-sw-industries-item-value"}
    )
    code = [item.get_text() for item in code_raw]
    name = [item.get_text().split("(")[0] for item in name_raw]
    parent_name = [
        item.find("span").get_text().split("(")[0][1:-1] for item in name_raw
    ]
    num = [item.get_text().split("(")[1].split(")")[0] for item in name_raw]
    num_1 = [
        item.find_all("span", attrs={"class": "value"})[0].get_text().strip()
        for item in value_raw
    ]
    num_2 = [
        item.find_all("span", attrs={"class": "value"})[1].get_text().strip()
        for item in value_raw
    ]
    num_3 = [
        item.find_all("span", attrs={"class": "value"})[2].get_text().strip()
        for item in value_raw
    ]
    num_4 = [
        item.find_all("span", attrs={"class": "value"})[3].get_text().strip()
        for item in value_raw
    ]
    temp_df = pd.DataFrame([code, name, parent_name, num, num_1, num_2, num_3, num_4]).T
    temp_df.columns = [
        "è¡Œä¸šä»£ç ",
        "è¡Œä¸šåç§°",
        "ä¸Šçº§è¡Œä¸š",
        "æˆä»½ä¸ªæ•°",
        "é™æ€å¸‚ç›ˆç‡",
        "TTM(æ»šåŠ¨)å¸‚ç›ˆç‡",
        "å¸‚å‡€ç‡",
        "é™æ€è‚¡æ¯ç‡",
    ]
    temp_df["æˆä»½ä¸ªæ•°"] = pd.to_numeric(temp_df["æˆä»½ä¸ªæ•°"], errors="coerce")
    temp_df["é™æ€å¸‚ç›ˆç‡"] = pd.to_numeric(temp_df["é™æ€å¸‚ç›ˆç‡"], errors="coerce")
    temp_df["TTM(æ»šåŠ¨)å¸‚ç›ˆç‡"] = pd.to_numeric(temp_df["TTM(æ»šåŠ¨)å¸‚ç›ˆç‡"], errors="coerce")
    temp_df["å¸‚å‡€ç‡"] = pd.to_numeric(temp_df["å¸‚å‡€ç‡"], errors="coerce")
    temp_df["é™æ€è‚¡æ¯ç‡"] = pd.to_numeric(temp_df["é™æ€è‚¡æ¯ç‡"], errors="coerce")
    return temp_df

def sw_index_third_cons(symbol: str = "801120.SI") -> pd.DataFrame:
    """æŠ“å–æŒ‡å®šç”³ä¸‡ä¸‰çº§è¡Œä¸šä¸‹çš„æ‰€æœ‰ä¸ªè‚¡æ•°æ®ï¼ˆå«è‚¡æ¯ç‡ï¼‰"""
    try:
        url = f"https://legulegu.com/stockdata/index-composition?industryCode={symbol}"
        r = requests.get(url, headers=headers, timeout=10)
        temp_df = pd.read_html(StringIO(r.text))[0]
        temp_df.columns = [
            "åºå·",
            "è‚¡ç¥¨ä»£ç ",
            "è‚¡ç¥¨ç®€ç§°",
            "çº³å…¥æ—¶é—´",
            "ç”³ä¸‡1çº§",
            "ç”³ä¸‡2çº§",
            "ç”³ä¸‡3çº§",
            "ä»·æ ¼",
            "å¸‚ç›ˆç‡",
            "å¸‚ç›ˆç‡ttm",
            "å¸‚å‡€ç‡",
            "è‚¡æ¯ç‡",
            "å¸‚å€¼",
            "å½’æ¯å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿(09-30)",
            "å½’æ¯å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿(06-30)",
            "è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿(09-30)",
            "è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿(06-30)",
        ]
        # æ•°æ®æ¸…æ´—
        temp_df["ä»·æ ¼"] = pd.to_numeric(temp_df["ä»·æ ¼"], errors="coerce")
        temp_df["å¸‚ç›ˆç‡"] = pd.to_numeric(temp_df["å¸‚ç›ˆç‡"], errors="coerce")
        temp_df["å¸‚ç›ˆç‡ttm"] = pd.to_numeric(temp_df["å¸‚ç›ˆç‡ttm"], errors="coerce")
        temp_df["å¸‚å‡€ç‡"] = pd.to_numeric(temp_df["å¸‚å‡€ç‡"], errors="coerce")
        # å¤„ç†è‚¡æ¯ç‡ï¼ˆå»æ‰%å¹¶è½¬æ•°å­—ï¼‰
        temp_df["è‚¡æ¯ç‡"] = temp_df["è‚¡æ¯ç‡"].astype(str).str.strip("%")
        temp_df["è‚¡æ¯ç‡"] = pd.to_numeric(temp_df["è‚¡æ¯ç‡"], errors="coerce")
        temp_df["å¸‚å€¼"] = pd.to_numeric(temp_df["å¸‚å€¼"], errors="coerce")
        
        # æ¸…ç†åŒæ¯”å¢é•¿å­—æ®µ
        for col in ["å½’æ¯å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿(09-30)", "å½’æ¯å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿(06-30)",
                    "è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿(09-30)", "è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿(06-30)"]:
            temp_df[col] = temp_df[col].astype(str).str.strip("%")
            temp_df[col] = pd.to_numeric(temp_df[col], errors="coerce")
        
        return temp_df
    except Exception as e:
        print(f"âŒ æŠ“å–è¡Œä¸š {symbol} å¤±è´¥ï¼š{e}")
        return pd.DataFrame()

def fetch_and_save_dividend_data():
    """ä¸»å‡½æ•°ï¼šéå†æ‰€æœ‰ç”³ä¸‡ä¸‰çº§è¡Œä¸šï¼ŒæŠ“å–å…¨Aè‚¡è‚¡æ¯ç‡å¹¶è¿½åŠ å†™å…¥CSV"""
    print("ğŸš€ å¯åŠ¨ä¹å’•ä¹è‚¡Aè‚¡è‚¡æ¯ç‡æŠ“å–ç¨‹åº...")
    csv_file = "dividend_data_shenwan.csv"
    csv_headers = ["ä»£ç ", "åç§°", "æœ€æ–°ä»·", "æ€»å¸‚å€¼(äº¿)", "è‚¡æ¯ç‡(%)", 
                   "ç”³ä¸‡1çº§", "ç”³ä¸‡2çº§", "ç”³ä¸‡3çº§", "å¸‚ç›ˆç‡ttm", "å¸‚å‡€ç‡"]
    
    # åˆå§‹åŒ–CSVæ–‡ä»¶ï¼ˆä¿è¯è¡¨å¤´å­˜åœ¨ï¼‰
    if not os.path.exists(csv_file):
        pd.DataFrame(columns=csv_headers).to_csv(
            csv_file, index=False, encoding='utf-8-sig'
        )
        print(f"ğŸ“„ åˆå§‹åŒ–CSVæ–‡ä»¶ï¼Œè¡¨å¤´ï¼š{csv_headers}")
    else:
        # æ£€æŸ¥è¡¨å¤´æ˜¯å¦åŒ¹é…
        try:
            df_check = pd.read_csv(csv_file, nrows=1)
            if list(df_check.columns) != csv_headers:
                df_old = pd.read_csv(csv_file, header=None)
                df_old.columns = csv_headers
                df_old.to_csv(csv_file, index=False, encoding='utf-8-sig')
                print(f"ğŸ”§ ä¿®å¤CSVè¡¨å¤´ä¸ºï¼š{csv_headers}")
        except:
            pd.DataFrame(columns=csv_headers).to_csv(
                csv_file, index=False, encoding='utf-8-sig'
            )
            print(f"ğŸ”§ é‡ç½®CSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´ï¼š{csv_headers}")
    
    # è®°å½•å·²æŠ“å–çš„è‚¡ç¥¨ä»£ç ï¼ˆå»é‡ï¼‰
    crawled_codes = set()
    if os.path.exists(csv_file):
        try:
            df_exist = pd.read_csv(csv_file)
            crawled_codes = set(df_exist["ä»£ç "].dropna().astype(str).tolist())
        except:
            crawled_codes = set()
    print(f"ğŸ“Œ å·²æŠ“å–è¿‡çš„è‚¡ç¥¨æ•°é‡ï¼š{len(crawled_codes)}")
    
    # è·å–æ‰€æœ‰ç”³ä¸‡ä¸‰çº§è¡Œä¸šä»£ç 
    third_industry_df = sw_index_third_info()
    third_industry_codes = third_industry_df["è¡Œä¸šä»£ç "].tolist()
    print(f"ğŸ“¥ å…±è·å– {len(third_industry_codes)} ä¸ªç”³ä¸‡ä¸‰çº§è¡Œä¸šï¼Œå¼€å§‹éå†æŠ“å–...")
    
    success_count = 0
    total_processed = 0
    
    for i, industry_code in enumerate(third_industry_codes):
        # æŠ“å–è¯¥è¡Œä¸šä¸‹çš„ä¸ªè‚¡æ•°æ®
        stock_df = sw_index_third_cons(symbol=industry_code)
        if stock_df.empty:
            time.sleep(0.5)  # å¤±è´¥æ—¶å»¶é•¿ç­‰å¾…
            continue
        
        # éå†è¯¥è¡Œä¸šä¸‹çš„ä¸ªè‚¡
        for _, row in stock_df.iterrows():
            code = str(row["è‚¡ç¥¨ä»£ç "]).strip()
            # è·³è¿‡å·²æŠ“å–çš„è‚¡ç¥¨ï¼ˆå»é‡ï¼‰
            if code in crawled_codes:
                continue
            
            # æå–æ ¸å¿ƒå­—æ®µ
            name = row["è‚¡ç¥¨ç®€ç§°"]
            current_price = row["ä»·æ ¼"]
            market_cap = row["å¸‚å€¼"]
            dividend_yield = row["è‚¡æ¯ç‡"]
            sw1 = row["ç”³ä¸‡1çº§"]
            sw2 = row["ç”³ä¸‡2çº§"]
            sw3 = row["ç”³ä¸‡3çº§"]
            pe_ttm = row["å¸‚ç›ˆç‡ttm"]
            pb = row["å¸‚å‡€ç‡"]
            
            # åªä¿ç•™æœ‰è‚¡æ¯ç‡ä¸”å¤§äº0çš„è®°å½•
            if pd.notna(dividend_yield) and dividend_yield > 0:
                single_data = pd.DataFrame({
                    "ä»£ç ": [code],
                    "åç§°": [name],
                    "æœ€æ–°ä»·": [current_price],
                    "æ€»å¸‚å€¼(äº¿)": [round(market_cap, 2) if pd.notna(market_cap) else 0],
                    "è‚¡æ¯ç‡(%)": [dividend_yield],
                    "ç”³ä¸‡1çº§": [sw1],
                    "ç”³ä¸‡2çº§": [sw2],
                    "ç”³ä¸‡3çº§": [sw3],
                    "å¸‚ç›ˆç‡ttm": [pe_ttm],
                    "å¸‚å‡€ç‡": [pb]
                })
                # è¿½åŠ å†™å…¥CSV
                single_data.to_csv(
                    csv_file,
                    mode='a',
                    index=False,
                    header=False,
                    encoding='utf-8-sig'
                )
                success_count += 1
                crawled_codes.add(code)  # æ ‡è®°ä¸ºå·²æŠ“å–
        
        total_processed += 1
        # æ‰“å°è¿›åº¦
        if i % 10 == 0:
            print(f"âœ… å·²å¤„ç† {total_processed}/{len(third_industry_codes)} ä¸ªè¡Œä¸šï¼Œæ–°å¢ {success_count} æ”¯æœ‰è‚¡æ¯çš„è‚¡ç¥¨...")
        time.sleep(0.3)  # é˜²åçˆ¬
    
    # æœ€ç»ˆæ’åºï¼ˆæŒ‰è‚¡æ¯ç‡é™åºï¼‰
    if os.path.exists(csv_file) and success_count > 0:
        df_final = pd.read_csv(csv_file)
        df_final = df_final.sort_values(by='è‚¡æ¯ç‡(%)', ascending=False).drop_duplicates(subset=["ä»£ç "], keep="first")
        df_final.to_csv(csv_file, index=False, encoding='utf-8-sig')
        print(f"\nâœ¨ ä»»åŠ¡å®Œæˆï¼ç´¯è®¡æŠ“å– {len(df_final)} æ”¯æœ‰è‚¡æ¯çš„Aè‚¡ï¼ˆå»é‡åï¼‰ã€‚")
        print(f"ğŸ“ æ•°æ®å·²å­˜å…¥ {csv_file}ï¼ŒæŒ‰è‚¡æ¯ç‡é™åºæ’åˆ—")
    else:
        print("âš ï¸ æœªæŠ“å–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ¥å£æ˜¯å¦æ­£å¸¸ã€‚")

if __name__ == "__main__":
    fetch_and_save_dividend_data()